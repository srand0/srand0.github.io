---
layout: article
title:  'SLAM!'
tags: SLAM
aside:
  toc: true
mathjax: true
cover: /image/posts/2019-12-29/0.architecture.png
---

![architecture-all](/image/posts/2019-12-29/0.architecture-all.png)

最近看了杨亮的课程视频，了解了SLAM涉及到的研究领域，包括基本理论、传感器、地图、位姿估计与融合、回路和图优化以及深度学习相关六大领域。

<!--more-->

SLAM(Simultaneous Localization And Mapping)于1986年由Peter Cheeseman, Jim Crowley和Hugh Durrant Whyte等人提出。在此之前，机器人的位置都是通过积分的方式来计算，随着时间的推移，累积误差会越来越大。在此之后，SLAM经历过三大里程碑式的发展：
- EKF：扩展卡尔曼滤波，在卡尔曼滤波的基础上，通过贝叶斯概率持续矫正机器人位姿。
- Fast-SLAM：使用粒子滤波算法，用概率统计的方法解决了KF假设噪声满足高斯分布的限制。
- Graph-SLAM：使用图优化方法进行定位。

目前定位使用的传感器主要分为视觉和激光两类：
- 视觉：最早使用视觉进行SLAM的是Javier Civera, Andrew J. Davision，当时使用的是EKF方法；后续Georg Klein, David Murray使用多线程图优化进行SLAM，主要项目是PTAM-SLAM（Parallel Tracking And Mapping），后续还出现了RGBD-SLAM、ORB-SLAM、LSD-SLAM等。
- 激光：早期是GMapping、SLAM6D等方法；后续Ji Zhang的LOAM、V-LOAM加入了特征提取进行SLAM；Google开源了Cartographer项目进行激光SLAM。


<!-- 
### 一、基本理论

![base-theory](/image/posts/2019-12-29/1.base-theory.png)


#### 1. 数学表示

##### 1）机器人位姿的表示：

在三维空间中，欧式坐标系使用P和R分别表示位置和朝向：

$$P(x,y,z)$$

$$R(\alpha,\beta,\gamma)$$

但是这样表示在后续的计算中不方便<span style="color:red;">（原因后续更新）</span>，所以在三维空间中一般使用四元数
$$q=a+bi+cj+dk$$
表示。
 -->

<!-- 
##### 2） 卡尔曼滤波（Kalman Filter）

卡尔曼滤波器又被称作传感器融合算法（Sensor Fusion Algorithm），它的原理就是将多个传感器的预测结果或者测量结果进行融合，得到最佳的结果，从而减少误差。其推算过程如下：

为了推算方便，我们先用一维状态进行推算，假设我们有两个传感器（里程计和激光）同时对一辆小车的前进距离$x$进行估计。两个传感器肯定都是有误差的，我们假设预测的结果分别服从高斯分布$\mathcal{N}(u_1, \sigma_1^2)$和$\mathcal{N}(u_2, \sigma_2^2)$ ：

$$f(x)=\frac{1}{\sqrt{2π}\sigma_1}e^{\frac{-(x-u_1)^2}{2\sigma_1^2}} $$

$$g(x)=\frac{1}{\sqrt{2π}\sigma_2}e^{\frac{-(x-u_2)^2}{2\sigma_2^2}} $$

即：
- 里程计预测的结果是小车前进的距离期望是$u_1$，方差为$\sigma_1^2$
- 激光预测的结果是小车前进的距离期望是$u_2$，方差为$\sigma_2^2$

这两个传感器的预测是相互独立的，根据$P(AB)=P(A)P(B)$（两个独立事件同时发生的概率为两个事件分别发生的概率的乘积）两个事件的最优估计的概率分布为：

$$f(x)*g(x)=\frac{1}{\sqrt{2π}\sigma_1}e^{\frac{-(x-u_1)^2}{2\sigma_1^2}} * \frac{1}{\sqrt{2π}\sigma_2}e^{\frac{-(x-u_2)^2}{2\sigma_2^2}} $$

计算后可以得到：
$$f(x)*g(x)=\frac{S_f}{\sqrt{2π}\sigma_f}e^{\frac{-(x-u_f)^2}{2\sigma_f^2}} $$

其中：

$$\sigma_f^2=\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2} $$

$$u_f=\frac{u_1\sigma_2^2+u_2\sigma_1^2}{\sigma_1^2+\sigma_2^2} $$

$$S_f=\frac{1}{\sqrt{2π(\sigma_1^2+\sigma_2^2)}}e^{\frac{-(u_1-u_2)^2}{2(\sigma_1^2+\sigma_2^2)}} $$

由此可以看出，两个高斯分布相乘后的结果还是一个高斯分布。由于$u_1, u_2, \sigma_1, \sigma_2 $都是固定值，因此$S_f$只是一个常数。

该过程即为两个传感器的估计结果融合的过程，融合后的结果为$N(u_f, \sigma_f)$，即两个传感器的结果融合后得到的最佳估计为$u_f$，最佳估计的方差为$\sigma_f^2$。

现在将$u_f$和$\sigma_f$进行变形，如下所示：

$$u_f=u_1 + \frac{\sigma_1^2(u_2-u_1)}{\sigma_1^2+\sigma_2^2}= u_1 + K*(u_2-u_1)$$

$$\sigma_f^2=\sigma_1^2-\frac{\sigma_1^4}{\sigma_1^2+\sigma_2^2}=\sigma_1^2-K*\sigma_1^2 $$

公式的物理意义即$u_f$的值等于传感器1的预测结果加上$K(u_2-u_1)$，假设传感器1（即里程计）的估计误差比较大，即方差比较大，$\sigma_1^2$比较大，那么$K$值就越大，最终$u_f$的结果就越偏向于$u_2$的值，假设$\sigma_1^2$趋近于无穷大，那么K就无限趋近于1，那么$u_f=u_2$，由此可见，如果$u_1$误差越大，$u_f$的结果中$u_1$所占的权重就越小；反之，如果$\sigma_2$越大，则$K$越趋近于0，$u_f$越趋近于$u_1$。这就是卡尔曼滤波器被称为传感器融合算法的原因，其中K就是最重要的用于调节两个传感器结果的系数。

下面我们从融合的结果推算卡尔曼滤波的五个公式：

我们已经得到了以下三个公式：

$$u_f = u_1 + K(u_2-u_1)$$

$$\sigma_f^2 = \sigma_1^2-K\sigma_1^2$$

$$ K = \frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2} $$

将这三个公式切换为矩阵的形式为：

$$K=\Sigma_0(\Sigma_0+\Sigma_1)^{-1}$$
$$\vec{\mu'} = \vec{\mu_0} + K(\vec{\mu_1} - \vec{\mu_0})$$
$$\Sigma' = \Sigma_0-K\Sigma_0$$

其中，使用$\mathcal{N}(\vec{\mu_0}, \Sigma_0)$替换$\mathcal{N}(u_1, \sigma_1^2)$，使用$\mathcal{N}(\vec{\mu_1}, \Sigma_1)$替换$\mathcal{N}(u_2, \sigma_2^2)$，得到的结果使用$\mathcal{N}(\vec{\mu'}, \Sigma')$替换$\mathcal{N}(u_f, \sigma_f^2)$。$(\Sigma_0+\Sigma_1)^{-1}$为矩阵$(\Sigma_0+\Sigma_1)$的逆矩阵，两者相乘后为单位矩阵。





下面我们将上一时刻的数据加入到计算中：

上面的公式中$\vec{\mu_0}$的物理意义为通过里程计以及上一时刻的位置而预测的当前时刻的位置。

我们用$\hat{x}_k$表示k时刻的机器人状态（位姿）最佳估计值，也是后验状态；用$\hat{x}_k^-$表示k时刻预测的机器人的状态值，也是先验状态。那么当前时刻的状态可以由上一时刻的状态经过线性变换后得到，即$\hat{x}_k^- = H_k \hat{x}_{k-1}$。

在使用一维状态进行计算时，状态使用高斯分布表示为：$\mathcal{N}(\mu, \sigma^2)$，物理意义为机器人前进的距离x的期望为$\mu$，方差为$\sigma^2$，在多维状态中，表示为矩阵形式的高斯分布为：$\mathcal{N}(\vec{\mu}, \Sigma)$，假设是三维状态，那么$\mu = \left[\begin{matrix}x_1\\x_2\\x_3 \end{matrix}\right]$，如果各个状态是不相关的，那么$\Sigma=\left[\begin{matrix} \sigma_1^2 & 0 & 0 \\ 0 & \sigma_2^2 & 0 \\ 0 & 0& \sigma_3^2\end{matrix}\right]$，但是现实中往往状态之间是有一定的相关性的，比如状态里面如果包含前进的距离和前进时的速度，那么速度的误差越大，往往距离的误差也就越大，两个状态就是正相关的，这种相关性可以使用协方差矩阵来表示，即$\Sigma_{ij}$表示状态$x_i$和状态$x_j$之间的关系，那么重新用矩阵表示方差为：$\Sigma=\left[\begin{matrix} \Sigma_{1,1} & \Sigma_{1,2} & \Sigma_{1,3} \\ \Sigma_{2,1} & \Sigma_{2,2} & \Sigma_{2,3} \\ \Sigma_{3,1} & \Sigma_{3,2}& \Sigma_{3,3}\end{matrix}\right]$。

上面我们通过矩阵$H_k$由上一时刻的后验状态$\hat{x}_{k-1}$得到了当前时刻的先验状态$\hat{x}_{k}^-$，那么当前时刻的协方差矩阵与上一时刻的协方差矩阵是什么关系呢？根据协方差矩阵的性质$Cov(Ax)=ACov(x)A^T$我们可以得到，当状态左乘$H_k$时，对应的协方差矩阵为原协方差矩阵左乘$H_k$并且右乘$H_k$的转置。由此，我们可以得到$\Sigma_k = H_k\Sigma_{k-1}H_k^T$。将协方差矩阵用$P$表示，我们可以将原来的高斯分布$\mathcal{N}(\vec{\mu_0}, \Sigma_0)$表示为：$\mathcal{N}(H_k\vec{\hat{x}_{k-1}}, H_kP_{k-1}H_k^T)$。

由此，我们已经将原公式转化为了（将$\hat{x}_k$、$\hat{x}_{k-1}$、$\hat{x}_{k}^-$、$H_kP_{k-1}H_k^T$、$P_k$带入公式）：

$$K=H_kP_{k-1}H_k^T(H_kP_{k-1}H_k^T+\Sigma_1)^{-1}$$
$$\hat{x}_k = H_k\hat{x}_{k-1} + K(\vec{\mu_1} - H_k\hat{x}_{k-1})$$
$$P_k = H_kP_{k-1}H_k^T-KH_kP_{k-1}H_k^T$$


下面我们将测量值数据加入计算中：

测量值得到的结果即当前的状态值，只是其存在误差，我们用$z$表示测量的状态值，用$R$表示其协方差矩阵，则原来的高斯分布$\mathcal{N}(\vec{\mu_1}, \Sigma_1)$转化为：$\mathcal{N}(z_k, R_k)$，代入公式后得到：

$$K_k=H_kP_{k-1}H_k^T(H_kP_{k-1}H_k^T+R_k)^{-1}$$
$$\hat{x}_k = H_k\hat{x}_{k-1} + K_k(z_k - H_k\hat{x}_{k-1})$$
$$P_k = H_kP_{k-1}H_k^T-K_kH_kP_{k-1}H_k^T$$

 -->


<!-- 
已知，$u_1$是根据里程计数据对前进距离的预测结果，也就是说$u_1$是个估计值，$u_2$是激光测量后得到的前进距离的结果，是个测量值，下面我们切换一下表达的方式。

假设状态使用$x$表示，当前的预测值由上一次的最优估计（后验估计）加上控制量$u$来进行线性预测：

$$\hat{x}_k^- = A\hat{x}_{k-1} + Bu_k $$

$\hat{x}$ 表示估计值，$^-$表示先验估计，在上面的例子中对应为在里程计按照速度$v$行驶$\Delta{t}$时间后预测当前的位置$u_{1,t} = u_{1,t-1} + v * \Delta{t}$。

假设测量值使用$y$表示，测量值与后验估计值之间存在线性关系，在上面的例子中对应为：假设上一时刻激光检测到前方障碍物距离为$y_{k-1}$，那么前进了$\Delta{x}$距离后，当前时刻检测到前方障碍物距离应当为$y_k = y_{k-1} - \Delta{x}$，即$y_k = - x_k + C$，在实际项目中，往往从测量值结果$y$转化到$x$不是很方便，我们就直接使用测量值带入公式进行计算：

$$y_k = H x_k$$ -->


---
<!-- 
未完待续 -->

<!-- 

### 二、传感器

![sensor](/image/posts/2019-12-29/2.sensor.png)


### 三、地图

![map](/image/posts/2019-12-29/3.map.png)


### 四、位姿估计与融合

![pose](/image/posts/2019-12-29/4.pose.png)


### 五、回路和图优化

![g2o](/image/posts/2019-12-29/5.g2o.png)


### 六、深度学习

![deep-learning](/image/posts/2019-12-29/6.deep-learning.png)



 -->
